# Code snippets
## Divergence estimation of continuous vector embeddings from `X(y=1)` and `X(y=0)`
- Implementation of Wang-Kulkarni-Verdu (2009) estimator of KL div. See Formula(25)
- Wang, Qing, Sanjeev R. Kulkarni, and Sergio Verdú. "Divergence estimation for multidimensional densities via $ k $-Nearest-Neighbor distances." IEEE Transactions on Information Theory 55.5 (2009): 2392-2405.
- `/snippets/mae_lit.py`: implementation in torch lightening format
- `/snippets/mae_kl_divergence.ipynb`: estimated KL div (k=3) during MTAE, measured on PTB-XL


# Slides
## Consistency Models.pdf
- Song, Yang, et al. "Consistency models." (2023).
## Dropout.pdf
- Hinton, Geoffrey E., et al. "Improving neural networks by preventing co-adaptation of feature detectors." arXiv preprint arXiv:1207.0580 (2012).
- Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks from overfitting." The journal of machine learning research 15.1 (2014): 1929-1958.
- Nalisnick, Eric, José Miguel Hernández-Lobato, and Padhraic Smyth. "Dropout as a structured shrinkage prior." International Conference on Machine Learning. PMLR, 2019.
## Graph Geometry Preserving Autoencoder
- Jang, Cheongjae, Yung-Kyun Noh, and Frank Chongwoo Park. "A Riemannian geometric framework for manifold learning of non-Euclidean data." Advances in Data Analysis and Classification 15.3 (2021): 673-699.
- Lee, Yonghyeon, Hyeokjun Kwon, and Frank Park. "Neighborhood reconstructing autoencoders." Advances in Neural Information Processing Systems 34 (2021): 536-546.
- Lee, Yonghyeon, et al. "Regularized autoencoders for isometric representation learning." International Conference on Learning Representations. 2022.
- Jang, Cheongjae, et al. "Geometrically regularized autoencoders for non-euclidean data." The Eleventh International Conference on Learning Representations. 2022.
- Lim, Jungbin, et al. "Graph geometry-preserving autoencoders." Forty-first International Conference on Machine Learning. 2024.
